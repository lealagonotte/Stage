##Méthode 1
import math
import numpy as np
import cmath
import scipy.stats as ss
import math
import matplotlib.pyplot as plt
import cmath


#global  -> coreset puis confidentialité

#Paramètres :
N=10
d=2
n=300
x = np.random.uniform(0, 1, n)
y = np.random.uniform(0, 1, n)

# Regrouper les coordonnées x et y dans un tableau numpy de dimension 2
vect_init = np.column_stack((x, y))







def generer_vecteurs(vectors):
    """BASE TRIGO
Crée et renvoie l'ensemble des points qu'on considère (de la forme (phi_j),j in [1, N] i in [1, n])
    vectors est l'ensemble des X_j donc une liste de liste pour la coordonnée c sachant qu'on est en dimension 2 donc i vaut soit 1 soit 0"""
    base_final=[]
    for k in range(len(vectors)) :
      vect=[]
      for i in range(1,N+1) :
        for j in range(1,N+1):
            if i==1 and j==1:
              vect.append(1)
            elif i==1 and j%2==0 :
              vect.append(math.sqrt(2)*math.sin(2*math.pi*(j//2)*vectors[k][1]))
            elif i==1 and j%2==1 :
              vect.append(math.sqrt(2)*math.cos(2*math.pi*(j//2)*vectors[k][1]))
            elif j==1 and i%2==0 :
              vect.append(math.sqrt(2)*math.sin(2*math.pi*(i//2)*vectors[k][0]))
            elif j==1 and i%2==1 :
              vect.append(math.sqrt(2)*math.cos(2*math.pi*(i//2)*vectors[k][0]))
            elif i%2==0 and j%2==0 :
              vect.append(2*math.sin(2*math.pi*(i//2)*vectors[k][0])*math.sin(2*math.pi*(j//2)*vectors[k][1]))
            elif i%2==0 and j%2==1 :
              vect.append(2*math.sin(2*math.pi*(i//2)*vectors[k][0])*math.cos(2*math.pi*(j//2)*vectors[k][1]))
            elif i%2==1 and j%2==0 :
              vect.append(2*math.sin(2*math.pi*(j//2)*vectors[k][1])*math.cos(2*math.pi*(i//2)*vectors[k][0]))
            elif i%2==1 and j%2==1 :
              vect.append(2*math.cos(2*math.pi*(j//2)*vectors[k][0])*math.cos(2*math.pi*(i//2)*vectors[k][1]))
      base_final.append(vect)
      #pour chaque X_i on crée le vecteur associé avec tous les phi_j
    return base_final




def generer_vecteurs2(vectors):
    """BASE DE HAAR
Crée et renvoie l'ensemble des points qu'on considère (de la forme (phi_j),j in [1, N] i in [1, n])
    vectors est l'ensemble des X_j donc une liste de liste pour la coordonnée c sachant qu'on est en dimension 2 donc i vaut soit 1 soit 0"""
    base_final=[]
    for k in range(len(vectors)) :
      vect=[]
      i=1
      while i<N+1 :
        j=1
        while j <N+1:
            if vectors[k][0]<=(i/N) and vectors[k][0]>((i-1)/N) and vectors[k][1]<=(j/N) and vectors[k][1]>((j-1)/N) :
              vect.append(N)
            else :
                vect.append(0)
            j+=1
        i+=1

      base_final.append(vect)
      #pour chaque X_i on crée le vecteur associé avec tous les phi_j
    return base_final



def create_matrix(*vectors):
    """
    Crée et renvoie la matrice M définie dans l'article

    Paramètres : vectors, ce sont les v_j qu'on met en paramètre, l'étoile permet de mettre un nombre d'arguments variables
    (le nombre de vecteurs diminuent au moins de 1 ) chaque étape


    """

    num_vectors = len(vectors)
    vector_size = len(vectors[0]) # Tous les vecteurs ont la meme taille

    # Création de la matrice
    matrix = np.zeros((vector_size +1, num_vectors))
    for i, vector in enumerate(vectors):
        matrix[:vector_size, i] = np.transpose(vector)
    #La dernirère ligne est composée de 1
    matrix[-1, :] = 1
    return matrix
def vecteur_propre(matrix):
    """
    Trouve et renvoie un vecteur non nul du noyau d'une matrice donnée en utilisant la décomposition en valeurs singulières (SVD).

    Arguments :
    - matrix : La matrice pour laquelle nous voulons trouver un vecteur non nul du noyau.

    """

    _, _, vh = np.linalg.svd(matrix)
    kernel_vector = vh[-1, :]

    return kernel_vector


def find_alpha( vector, lambd):
    """Trouve et renvoie un vecteur alpha qui annule une des composantes du vecteur appelé lambda dans l'article*
    il faut trouver alpha de telle sorte que les autres coefficients ne soient pas négatifs donc annuler le minimum en fait doit marche
    Paramètres :

    -vector : vector est un vecteur du noyau calculé grâce à la fonction précédente"""
    old=math.inf
    i=0
    j=0
    while i<len(vector) :
        if vector[i]!=0 :

            a = (-lambd[i]  /vector[i])

            if abs(a)<abs(old)  :
                old=a
                j=i
        i+=1
    if old==math.inf :
      old=np.min(lambd)
    v=old * vector
    #print(len(v))
    #print(len(lambd))
    res = lambd+ v
    return res,j #renvoie le lambda_i et l'indice de la composante qu'on a annulé

def iteration(borne, *vectors, ite,lambfin, indice, lambd_old, ) :
    """ Itère les deux étapes jusqu'à ce qu'on ait plus que D+1 vecteurs
    Paramètrees :
    -borne est la taille de coreset qui permet d'être assez proche de la vitesse q'on avait avec l'estimateur basé sur toutes les données initiales
    -iter : indique à quelle itération on est
    -lambdfin -> fait le produit des lambda i au fur et à mesure, ce sont ces coefficients que l'on cherche à avoir
    -indice : tableau de 0 ou de 1. Si indice[j]=0 alors on a annulé le coeff devant la composante j sinon elle est encore là"""
    M=create_matrix(*vectors) #on crée la matrice

    nb=len(vectors) #nb de vecteurs qu'il nous reste
    ite+=1 # pour savoir combien d'itération on a déjà fait

    #Trouve le vecteur du noyau non nul de M
    v=vecteur_propre(M)

    #Donne lambda_j et la composante qu'on annule
    (lambd,j)=find_alpha(v, lambd_old)

    # Met à jour le tableau de vecteur (pour les histoires de moyenne, on doit multiplier les nvx vecteurs par n-iter)

    vect = []
    lambd_new=[]
    for i in range(len(vectors)):
        v = np.array(vectors[i], dtype=np.float64)

        if i != j:  # Exclude the element at index j
            vect.append(v)
            lambd_new.append(lambd[i])
    # Enlève la composante j car c'est celle qu'on a annulé, on réduit ainsi le nb de vecteurs à l'étape suivante
    #print(indice)
    lambd_new=np.array(lambd_new, dtype=np.float64)
    rg=0
    for k in range(len(lambd)) :
        if indice[rg]==0 :
            while indice[rg]== 0  :
                rg+=1
        lambfin[rg]=lambd[k]
        if j==k :
            lambfin[rg]=0
            indice[rg]=0

        rg+=1

    #Met à jour le tableau des lambdfin car on voit qu'en itérant les poids finaux seront le produit des lambda_i à chaque itération i
    if nb>borne : #tant que le nb est supérieur à D+1 on peut encore en supprimer par le théorème de Carathéodory  donc on itére
        iteration(borne,*vect, ite=ite, lambfin=lambfin, indice=indice, lambd_old=lambd_new)
    return indice










def taille_coreset(vecteurs) :
    """trouve la taille de coreset la plus petite possible telle que l'estimateur du coreset soit assez proche de la vraie densité4

    A chaque itération, on va augmenter la taille du coreset de 1 sachant que au minimum, la taille sera de N**d+1"""

    #on va calculer les normes pour les estimateurs

    borne = N**2+1
    while borne<n :
        norme2=0
        A=0
        #on reconstruit l'estimateur comme dans la fonction estimateur
        alpha_est=[0 for j in range(N**2)] #celle ou on a réduit le nombre de données
        alpha=[0 for j in range(N**2)] #alpha total

    #on crée les vecteurs qu'on considère (les (phi_j(x_i))j, i)


    #on applique l'algorithme de Carathéodory à nos vecteurs (pour cette partie c 'est vraiment similaire à ce qu'on a fait avec les estimateurs à noyaux)
        coeff=iteration(borne,*vecteurs, ite= 0, lambfin = [1 for i in range(n)], indice=[1 for i in range(n)], lambd_old=[1/n for i in range (n)] )
        for i in range(N**2) :
        #on calcule les alpha (alpha_i,j = 1/n * somme(v) et les alpha_i,j = somme(nouveaux coeff * v))
            sum2=0
            sum1=0
            for j in range(len(vecteurs)):

                sum1+=vecteurs[j][i]*(1/n)
                sum2+=vecteurs[j][i]/(sum(coeff)) #on considère l'estimateur à poids uniforme
            alpha[i]=sum1
            alpha_est[i]=sum2


        Xj = np.array(vecteurs)

        x = np.linspace(0,1, 100)
        y = np.linspace(0,1, 100)

        # Générer la grille de points
        X, Y = np.meshgrid(x, y)
        pos = np.column_stack((X.ravel(), Y.ravel()))  # Flatten X and Y to generate pos
    # Calculer les densités de probabilité pour chaque point de la grille
        Z = []
        Z2=[]
        #print(pos)
        P=generer_vecteurs(pos)
        #on reconstruit l'estimateur à partir des alpha

        for k in range(len(P)) :
            z = 0
            z2=0
            for i in range(len(alpha)):
                z +=  alpha[i]*P[k][i]
                z2+=alpha_est[i]*P[k][i]
                norme2+=1/len(pos)*((z2-z)**2)
                A+=1/len(pos)*((1-z)**2)
                Z.append(z)
                Z2.append(z2)
        print("A", A,"norme2", norme2)
        if norme2<=A :
            #print(norme2-A)
            return(borne, coeff)
        else :
            print(borne)
            borne+=1
    return "rien"


alpha2=1

def anonymise(vecteurs) :
    """on confidentiale l'échantille initial en rajoutant du bruit
    Paramètres :
    data : échantillon de données initiales
    sigma : variance, paramètre devant les W_ij
    W : suit une loi de Laplace, on les somme aux données pour rajouter du bruit
    """
    alpha_est=[0 for j in range(N**2)] #celle ou on a réduit le nombre de données
    alpha=[0 for j in range(N**2)] #alpha total

    #on crée les vecteurs qu'on considère (les (phi_j(x_i))j, i)
    W=np.random.laplace(0,1,N**2)

    (borne, coeff)=taille_coreset(vecteurs)
    sigma=2*N/(borne*alpha2  )
    #on applique l'algorithme de Carathéodory à nos vecteurs (pour cette partie c 'est vraiment similaire à ce qu'on a fait avec les estimateurs à noyaux)
    coeff=iteration(borne,*vecteurs, ite= 0, lambfin = [1 for i in range(n)], indice=[1 for i in range(n)], lambd_old=[1/n for i in range (n)] )
    for i in range(N**2) :
        #on calcule les alpha (alpha_i,j = 1/n * somme(v) et les alpha_i,j = somme(nouveaux coeff * v))
        sum2=0
        sum1=0
        for j in range(len(vecteurs)):

            sum1+=vecteurs[j][i]*(1/n)
            if coeff[j]!=0 :
                sum2+=vecteurs[j][i]/(sum(coeff)) #on considère l'estimateur à poids uniforme
        alpha[i]=sum1
        alpha_est[i]=sum2


    Xj = np.array(vecteurs)

    x = np.linspace(0,1, 100)
    y = np.linspace(0,1, 100)

        # Générer la grille de points
    X, Y = np.meshgrid(x, y)
    pos = np.column_stack((X.ravel(), Y.ravel()))  # Flatten X and Y to generate pos
    # Calculer les densités de probabilité pour chaque point de la grille
    Z = []
    Z2=[]
    #print(pos)
    P=generer_vecteurs2(pos)
        #on reconstruit l'estimateur à partir des alpha
    
    for k in range(len(P)) :
        z = 0
        z2=0
        for i in range(len(alpha)):
            z +=  alpha[i]*P[k][i]
            z2+=(alpha_est[i]+sigma*W[i])*P[k][i]

        Z.append(z)
        Z2.append(z2)
    
    Z = np.reshape(Z2, X.shape)
    plt.pcolormesh(X, Y,Z, shading='auto', cmap='viridis')

    plt.colorbar(label='Densité de probabilité')
    plt.xlabel('Variable 1')
    plt.ylabel('Variable 2')
    plt.title('Représentation d\'une loi multivariée en dimension 2')
    plt.show()



anonymise(generer_vecteurs2(vect_init))
